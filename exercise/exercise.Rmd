---
title: "Tutorial"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.cap = "Sandbox")
library(tidyverse)
library(saqgetr)
library(worldmet)
library(openair)
library(ricardor)

spain_sites <- get_saq_sites() %>%
  filter(country == "spain")

es_aq <- get_saq_observations(
  site = c("es1422a", "es0115a"),
  start = "2019-07-01",
  end = "2019-07-31"
)

spain_sites_simple <- spain_sites %>%
  select(site, site_name, latitude, longitude, site_type)

es_aq_join <- es_aq %>%
  left_join(spain_sites_simple)

es_aq_wide <- es_aq_join %>%
  saq_clean_observations(summary = "hour", valid_only = TRUE, spread = TRUE)

met <- readRDS("met.rds")


```


## Getting AQ data

All the packages required for this tutorial have already been loaded. 

### Finding available data

* Find all the available monitoring sites in Spain and save it to a data frame called `spain_sites`
* Quickly view a summary of `spain_sites` with `glimpse()`

```{r get-data, exercise=TRUE, }

```

```{r get-data-hint-1}

spain_sites <- get_saq_sites() %>%
  filter(country == "spain")

```

```{r get-data-hint-2}

glimpse(spain_sites)

```

### Plot all the Spanish sites

Now plot the sites on a map and find two sites that are still in operation in 2019.

* First you will need to convert the data frame to a spatial object with `sp_from_data_frame()`
* Then plot it with `plot_leaflet()`

```{r plot-site, exercise=TRUE}

```

```{r plot-site-solution}
spain_sites %>%
  mutate(date_end = as.character(date_end)) %>% # convert date to character to allow it show up properly on the map
  sp_from_data_frame(type = "point", 
                     latitude = "latitude", longitude = "longitude") %>%
  plot_leaflet(popup = c("site", "site_name", "site_type", "date_end"),
               radius = 8, colour = "red", clusterOptions = leaflet::markerClusterOptions()) # try different options in plot_leaflet to get the desired plot

```

```{r plot-site-hint-1}

# first we need to convert the date_end column to character if we to display it on the map
spain_sites %>%
  mutate(date_end = as.character(date_end)) %>%
  sp_from_data_frame(...) %>%
  plot_leaflet(...)

```

```{r plot-site-hint-2}

# To plot the data on a map, first we need to convert the data frame to a spatial object using sp_from_data_frame
spain_sites %>%
  mutate(date_end = as.character(date_end)) %>%
  sp_from_data_frame(type = "piont") %>%
  plot_leaflet(...)

```

### Select two sites

Zoom into Madrid and find two sites that are still in operation in 2019.

Filter the `spain_sites` data frame to get info about two sites in Madrid:

* Plaza del Carmen (es1422a)
* Plaza de Espana (es0115a)

```{r two-sites, exercise=TRUE}


```

```{r two-sites-hint-1}

# %in% operator in R, is used to identify if an element belongs to a vector

spain_sites %>%
  filter(... %in% c(...))

```

```{r two-sites-hint-2}

spain_sites %>%
  filter(site %in% c("es1422a", "es0115a"))

```


### Get AQ data

* Now let's download July 2019 data for these sites with `get_saq_observations()` function. 
* Save the output to `es_aq`

```{r get-aq, exercise=TRUE}

```

> Note that when you assign the output to a data frame, you will NOT see any output in the console.

```{r get-aq-hint-1}

# select two sites in Madrid
es_aq <- get_saq_observations(
  site = c("es1422a", "es0115a"),
  start = "2019-07-01",
  end = "2019-07-31"
)

```

## Data exploration

Run the code below to get a quick look at the `es_aq` data frame.

```{r glimpse-es_aq, exercise=TRUE}

glimpse(es_aq)

```

The AQ data downloaded contain 9 columns.

* `date` the start date/time of the measurement
* `end_date` the end date/teim of the measurement
* `site` the site code
* `variable` the pollutant name
* `process` the process code. Each site, variable, measurement period combination has a unique process code.
* `summary` the measurement period. `1` mean hourly measurement. To get all the availabel periods and their corresponding codes, use `get_saq_summaries()` function.
* `validity` measurement validity. `1` means valid. To get all the validity code, use `get_saq_validity()`
* `unit` measurement unit
* `value` actual measurement

### Data joining

Right now the data frame only contains site code but not site name. This may be difficult to read for the following analysis. Let's join some info about the sites from the `spain_sites` data frame to the `es_aq` measurement.

First let's select what we want to join. To get the column names of a data frame use `names()`.

```{r get-names, exercise=TRUE}
names(es_aq)
```

We need at least `site`, `site_name`.  `latitude`, `longitude` and `site_type` can also be useful.

Let's create a new data frame with only thse columns and name it `spain_sites_simple`. We will use the `select` function. Edit the code below to select these 5 columns.

```{r select, exercise=TRUE}

spain_sites_simple <- spain_sites %>%
  select(...)
  
```

Now we can use `left_join()` to join them together.

```{r left-join, exercise=TRUE}

es_aq_join <- es_aq %>%
  left_join(spain_sites_simple, by = "site")

```

Now let's take a look at the data frame `es_aq_join` again using `glimpse()`

```{r glimpse-es-aq-join, exercise=TRUE}

```

### Data summary

It's time to get some summaries of dataset. First let's see what pollutants are measured at each site and how many measurement there are for each of them. Getting the number of measurement is simple with `count()`  function. But how do we count by each site and pollutant combination. We need a way to group the dataset into different chunks and do the count operation for each group. Conveniently this can be achieved by `group_by()` function.

* First group the `es_aq_join` data frame by `site_name` and `variable`
* Count the number of measurement for each group

```{r group, exercise=TRUE}

  

```


```{r group-hint-1}

es_aq_join %>%
  group_by(site_name, variable)

```

```{r group-hint-2}

es_aq_join %>%
  group_by(site_name, variable) %>%
  count()

```

In the previous exercise, no argument is supplied to the `count()` function. This way it counts all the columns. You can also supply a column name to `count()` and it will group the data frame by that column and then do a count for each group. Now let's try finding out the number of valid and invalid measurements by using the the `validity` column.

```{r validity, exercise=TRUE}


```

```{r validity-hint-1}

es_aq_join %>%
  count(validity)

```

Finally let's get some more interesting statistics. What is the mean for each pollutant at each site for the downloaded period. As before, we want to group the data by `site_name` and `variable` and using `mean()` to calculate the average. However `mean()` can't take grouped data frame, it only takes a vector. We need some function that takes a data frame and allows to do some operations inside it. Introducing the `summarise()` function. As the name suggests, it takes a data frame and creates new columns that summarise existing variables. The syntax is as follows.

```{r summarise-example, echo=TRUE, eval=FALSE}

df %>%
  group_by(variable) %>%
  summarise(ave = mean(col_name)) 

# ave is the new column name of the summary result
# mean is the summarising function
# col_name is the existing column name in df you want to summarise

```

Now let's try summarising the mean of each pollutant at each site.

```{r summarise, exercise=TRUE}

es_aq_join %>%
  group_by(...) %>%
  summarise(...)

```

```{r summarise-hint-1}

es_aq_join %>%
  group_by(site_name, variable) %>%
  summarise(ave = mean(value))

```

### Data reshaping

Currently the data we have is in a long format, i.e. all the pollutant names are in one column. Sometimes we want them in a wide format, i.e. all the pollutants have its own column filled with values. The wide format is generally prefered by functions in the `openair` package. To reshape the data, `saq_clean_observations()` can get this done quite easily. There're some other useful arguments baked in that let's you filter the hourly and valid data only.

```{r reshape, excercise=TRUE}

es_aq_wide <- es_aq_join %>%
  saq_clean_observations(summary = "hour", valid_only = TRUE, spread = TRUE)

glimpse(es_aq_wide)

```


### Data averaging

Now let's try another (probably easier) way to get the average value for a time series data using the `timeAverage()` function in `openair`. The syntax is very simple.

```{r timeave-example, echo=TRUE, eval=FALSE}

timeAverage(mydata, avg.time = "day", statistic = "mean", type = "site",
            data.thresh = 75)

```

Some explanations:

* `avg.time` lets you choose the averaging period.
* `statistic` lets you choose what summarising method to use. This can be "mean", "median", "max", "min", etc. More details see the `timeAverage` help function.
* `type` allows you to apply the averaging to different groups if they exist. Typically this refers to different sites in the data frame.
* `data.thresh` allows you specify a data capture threshold when averaging the data. For daily average, a value of 100 means all 24 valid measurements need to be present (i.e. not missing or `NA`) to calculate a daily average value. Otherwise it will be `NA`.

Now let's try calculating the daily average of all the pollutants at each site. Note that we need to use the wide data created in the previous exercise.

```{r timeave, exercise=TRUE}

timeAverage(...)

```

```{r timeave-hint-1}

timeAverage(es_aq_wide, avg.time = "day", statistic = "mean", type = "site")

```

## Getting met data

### Find the met station

Met data is often essential for analysing air quality data. `worldmet` package makes it easy to download surface meteorological data around the world. First let's check which station we need to download the data from for the two monitoring sites in Madrid. We will use the `getMeta()` function to find the closest met station. `getMeta()` takes a pair of latitude and longitude coordinates and plots the nearest 10 stations on a map. Remember you can find the coordinates for the monitoring sites by looking at the `es_aq_join` data frame.

```{r getmeta, exercise=TRUE}


```

```{r getmeta-hint-1}

glimpse(es_aq_join)

```

```{r getmeta-hint-2}

getMeta(lat = 40.42417, lon = -3.712222)

```

### Download met data

From the map above, we find that Madrid Retiro is the closest met station. We will need its site code (082220-99999) to download the met data. Now try to download data for this site for 2019.

```{r download-met, exercise=TRUE}

met <- imoprtNOAA(code = ..., year = ...)

```


### Join met data with AQ data

Now that we have the met data, we can join it with the AQ data for further analysis using a suite of `openair` functions. Remember the syntax for `left_join` is as follows.


```{r join-example, echo=TRUE, eval=FALSE}

left_join(df1, df2, by = "col_name")

```

Now try to join the `es_aq_wide` data frame with `met` data frame and save the output to `es_aq_wide`.

```{r join-met, exercise=TRUE}



```

```{r join-met-hint}

es_aq_wide <- left_join(es_aq_wide, met, by = "date")

```

> Note that the order of two data frames in `left_join` matters. `left_join` will keep all the rows and columns of the first data frame and matching results from the second data frame. Other variants of joining are available. You can find more details with `?join`


